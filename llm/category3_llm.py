import os
from glob import glob
from base_processor import BaseProcessor
from llm.chat_context_manager import (
    get_recent_qa_pairs,
    compress_conversation_context_simple,
    expand_user_question_simple,
    query_gemini_simple
)
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
load_dotenv()

# Kategori bilgileri
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))  # Ana klasÃ¶re git
CATEGORY_NAME = os.path.join(PROJECT_ROOT, "data", "AI_strategy")
DATA_DIR = CATEGORY_NAME

# Dosya desenleri
FILE_PATTERNS = ["*.pdf"]


# Gemini yapÄ±landÄ±rmasÄ±
GEMINI_CONFIG = {
    #"url_base": "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=GEMINI_API_KEY",
    "model": "gemini-2.0-flash",
    "gemini_api_key": os.getenv("GEMINI_API_KEY", "AIzaSyA3JU7tkx8Fcc_S2sIliZqIGSwlX1ZOWQY")}

# QA Prompt (KullanÄ±cÄ± tarafÄ±ndan dÃ¼zenlenecek)
QA_PROMPT = """
            Sen bir Yapay Zeka Stratejisi (AI Strategy) uzmanÄ±sÄ±n. GÃ¶revin, kullanÄ±cÄ±nÄ±n sorduÄŸu sorularÄ± sana verilen belgelerdeki bilgiler Ä±ÅŸÄ±ÄŸÄ±nda dikkatle araÅŸtÄ±rmak ve anlamlÄ±, kapsamlÄ±, doÄŸru cevaplar Ã¼retmektir.

            Cevap verirken ÅŸu noktalara dikkat etmelisin:
            1- Sorulan sorunun cevabÄ± eÄŸer elimdeki belgelerde yoksa asla uydurma veya yanlÄ±ÅŸ bilgi verme. BÃ¶yle durumlarda, lÃ¼tfen ÅŸu ÅŸekilde yanÄ±tla: 
            "ÃœzgÃ¼nÃ¼m, bu soruya mevcut belgeler Ä±ÅŸÄ±ÄŸÄ±nda cevap veremiyorum. LÃ¼tfen daha detaylÄ± sorun veya farklÄ± bir soru sorunuz."
            2- VereceÄŸin cevaplar, kullanÄ±cÄ±nÄ±n sorusuna doÄŸrudan ve aÃ§Ä±k ÅŸekilde odaklanmalÄ±, anlaÅŸÄ±lÄ±r, samimi ve bilgilendirici olmalÄ±dÄ±r.
            3- Gerekli durumlarda cevabÄ±nÄ± gÃ¼Ã§lendirmek iÃ§in tablolar, gÃ¶rseller veya belgelerden alÄ±ntÄ±lar ekleyebilirsin.
            4- EÄŸer kullanÄ±cÄ± senden iki veya daha fazla Ã¼lkenin yapay zeka stratejisini karÅŸÄ±laÅŸtÄ±rmanÄ± isterse, cevabÄ±nÄ± kÄ±yaslama yapacaÄŸÄ±n baÅŸlÄ±klara gÃ¶re bÃ¶l; her baÅŸlÄ±k altÄ±nda Ã¼lkeleri ayrÄ± ayrÄ± kÄ±yasla ve yanÄ±tÄ±nÄ±n en sonuna bir kÄ±yaslama tablosu ekle.

            AÅŸaÄŸÄ±da, AI_strategy kategorisindeki belgelerden alÄ±nan ilgili bÃ¶lÃ¼mler yer alÄ±yor:

            {context}

            KullanÄ±cÄ±nÄ±n sorusu: {question}
"""

# BaseProcessor'Ä± baÅŸlat
processor = BaseProcessor(
    index_root="faiss_indexes",
    model_name="",
    qa_prompt=QA_PROMPT,
    google_config=GEMINI_CONFIG
)

def load_and_index_documents():
    """AI_strategy kategorisi iÃ§in dÃ¶kÃ¼manlarÄ± yÃ¼kler ve FAISS indeksini oluÅŸturur/yÃ¼kler."""
    processor.get_or_create_index("AI_strategy", DATA_DIR)

def get_available_documents() -> list[str]:
    """Veri dizinindeki dÃ¶kÃ¼man adlarÄ±nÄ± listeler."""
    names = []
    for pattern in FILE_PATTERNS:
        for fp in glob(os.path.join(DATA_DIR, pattern)):
            names.append(os.path.basename(fp))
    return names

API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyA3JU7tkx8Fcc_S2sIliZqIGSwlX1ZOWQY")

# def get_qa_response(user_question: str, chat_history: list[dict] | None = None) -> dict:
#     if chat_history is None:
#         chat_history = []

#     recent_qa = get_recent_qa_pairs(chat_history)

#     compressed_context = compress_conversation_context_simple(recent_qa)

#     expanded_question = expand_user_question_simple(user_question, compressed_context, API_KEY)

#     retriever = processor.get_retriever("AI_strategy", DATA_DIR)
#     retrieved_docs = retriever.get_relevant_documents(expanded_question)
#     retrieved_docs = retriever.get_relevant_documents(expanded_question)
#     print(f"Retrieved {len(retrieved_docs)} documents.")
#     for doc in retrieved_docs:
#       print(f"\nğŸ“„ Retrieved from: {doc.metadata['source']}")
#       print(doc.page_content[:500])  # Ä°lk 500 karakteri gÃ¶ster, gerekirse artÄ±r


#     doc_context = "\n\n".join([
#         f"Document ({doc.metadata['source']}):\n{doc.page_content}"
#         for doc in retrieved_docs
#     ])

#     final_prompt = QA_PROMPT.format(context=doc_context, question=expanded_question)

#     prompt_template = PromptTemplate(
#         input_variables=["context", "question"],
#         template=QA_PROMPT
#     )

#     llm = ChatGoogleGenerativeAI(
#         model=processor.google_config["model"],
#         google_api_key=processor.google_config["gemini_api_key"],
#         temperature=0.5
#     )

#     qa_chain = RetrievalQA.from_chain_type(
#         llm=llm,
#         chain_type="stuff",
#         retriever=retriever,
#         return_source_documents=True,
#         chain_type_kwargs={"prompt": prompt_template}
#     )

#     result = qa_chain.invoke({"query": expanded_question})

#     answer = result.get("result", "")
#     source_docs = result.get("source_documents", [])
#     sources = list(set(doc.metadata.get("source", "") for doc in source_docs))

#     return {"answer": answer, "sources": sources}
def get_qa_response(user_question: str, chat_history: list[dict] | None = None) -> dict:
    if chat_history is None:
        chat_history = []

    recent_qa = get_recent_qa_pairs(chat_history)

    compressed_context = compress_conversation_context_simple(recent_qa)

    expanded_question = expand_user_question_simple(user_question, compressed_context, API_KEY)
    #expanded_question = user_question  # geÃ§ici olarak orijinal soruyu kullan


    retriever = processor.get_retriever("AI_strategy", DATA_DIR)
    retrieved_docs = retriever.get_relevant_documents(expanded_question)
    retrieved_docs = retriever.get_relevant_documents(expanded_question)
    print(f"Retrieved {len(retrieved_docs)} documents.")
    for doc in retrieved_docs:
      print(f"\nğŸ“„ Retrieved from: {doc.metadata['source']}")
      print(doc.page_content[:500])  # Ä°lk 500 karakteri gÃ¶ster, gerekirse artÄ±r


    doc_context = "\n\n".join([
        f"Document ({doc.metadata['source']}):\n{doc.page_content}"
        for doc in retrieved_docs
    ])

    final_prompt = QA_PROMPT.format(context=doc_context, question=expanded_question)

    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template=QA_PROMPT
    )

    llm = ChatGoogleGenerativeAI(
        model=processor.google_config["model"],
        google_api_key=processor.google_config["gemini_api_key"],
        temperature=0.5
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt_template}
    )

    result = qa_chain.invoke({"query": expanded_question})

    answer = result.get("result", "")
    source_docs = result.get("source_documents", [])
    sources = list(set(doc.metadata.get("source", "") for doc in source_docs))

    return {"answer": answer, "sources": sources}
if __name__ == "__main__":
    print("ğŸ” AI_strategy belgeleri indeksleniyor...")
    load_and_index_documents()
    print("âœ… TamamlandÄ±.")
    print("\nğŸ§  Soru soruluyor...")
    result = get_qa_response("Ä°ngilterenin yapay zeka stratejisi nedir?")
    print("\nğŸ“˜ YanÄ±t:")
    print(result["answer"])
    print("\nğŸ“„ Kaynaklar:")
    print(result["sources"])

